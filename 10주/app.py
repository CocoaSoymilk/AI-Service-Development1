import os
import pathlib
import streamlit as st

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain.vectorstores import FAISS
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories.streamlit import StreamlitChatMessageHistory

###############################################################
# OpenAI API Key ì„¤ì • (í™˜ê²½ë³€ìˆ˜ ì‚¬ìš© ê¶Œì¥)
###############################################################
os.environ.setdefault("OPENAI_API_KEY", "YOUR_OPENAI_API_KEY")

###############################################################
# PDF ë¡œë“œ & ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶• (FAISS)
###############################################################

@st.cache_resource(show_spinner=False)
def load_and_split_pdf(file_path: str):
    """PDF ë¥¼ ë¡œë“œí•´ LangChain ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    loader = PyPDFLoader(file_path)
    return loader.load_and_split()


@st.cache_resource(show_spinner=False)
def create_vector_store(_docs):
    """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© í›„ FAISS ë²¡í„°ìŠ¤í† ì–´ ìƒì„±, ë¡œì»¬ ì €ì¥"""
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    split_docs = text_splitter.split_documents(_docs)

    persist_directory = pathlib.Path("./faiss_db")
    persist_directory.mkdir(parents=True, exist_ok=True)

    vectorstore = FAISS.from_documents(
        split_docs,
        OpenAIEmbeddings(model="text-embedding-3-small"),
    )
    # ë””ìŠ¤í¬ì— ì €ì¥ (index.faiss & index.pkl)
    vectorstore.save_local(str(persist_directory))
    return vectorstore


@st.cache_resource(show_spinner=False)
def get_vectorstore(_docs):
    """ì´ë¯¸ ì €ì¥ëœ FAISS ì¸ë±ìŠ¤ê°€ ìˆìœ¼ë©´ ë¶ˆëŸ¬ì˜¤ê³ , ì—†ìœ¼ë©´ ìƒˆë¡œ ë§Œë“­ë‹ˆë‹¤."""
    persist_directory = pathlib.Path("./faiss_db")
    index_file = persist_directory / "index.faiss"

    if index_file.exists():
        try:
            return FAISS.load_local(
                str(persist_directory),
                OpenAIEmbeddings(model="text-embedding-3-small"),
            )
        except Exception:
            # ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ìƒˆë¡œ ìƒì„±
            pass
    return create_vector_store(_docs)

###############################################################
# RAG ì²´ì¸ ì´ˆê¸°í™”
###############################################################

@st.cache_resource(show_spinner=False)
def initialize_components(selected_model: str):
    file_path = r"../data/ëŒ€í•œë¯¼êµ­í—Œë²•(í—Œë²•)(ì œ00010í˜¸)(19880225).pdf"
    pages = load_and_split_pdf(file_path)
    vectorstore = get_vectorstore(pages)
    retriever = vectorstore.as_retriever()

    # ì±„íŒ… íˆìŠ¤í† ë¦¬ ìš”ì•½ìš© ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    contextualize_q_system_prompt = (
        """Given a chat history and the latest user question which might reference context in the chat history, "
        "formulate a standalone question which can be understood without the chat history. "
        "Do NOT answer the question, just reformulate it if needed and otherwise return it as is."""
    )

    contextualize_q_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", contextualize_q_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    # ì§ˆë¬¸â€‘ë‹µë³€ í”„ë¡¬í”„íŠ¸
    qa_system_prompt = (
        """You are an assistant for questionâ€‘answering tasks. Use the following pieces of retrieved context to answer the "
        "question. If you don't know the answer, just say that you don't know. Keep the answer perfect. please use imogi "
        "with the answer. ëŒ€ë‹µì€ í•œêµ­ì–´ë¡œ í•˜ê³ , ì¡´ëŒ“ë§ì„ ì¨ì¤˜.\n\n{context}"""
    )

    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            MessagesPlaceholder("history"),
            ("human", "{input}"),
        ]
    )

    llm = ChatOpenAI(model=selected_model)

    history_aware_retriever = create_history_aware_retriever(
        llm, retriever, contextualize_q_prompt
    )
    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
    return rag_chain

###############################################################
# Streamlit UI
###############################################################

st.header("í—Œë²• Q&A ì±—ë´‡ ğŸ’¬ ğŸ“š")
option = st.selectbox("Select GPT Model", ("gpt-4o-mini", "gpt-3.5-turbo-0125"))

rag_chain = initialize_components(option)
chat_history = StreamlitChatMessageHistory(key="chat_messages")

conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    lambda session_id: chat_history,
    input_messages_key="input",
    history_messages_key="history",
    output_messages_key="answer",
)

# ì´ˆê¸° ë©”ì‹œì§€
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        {
            "role": "assistant",
            "content": "í—Œë²•ì— ëŒ€í•´ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”!",
        }
    ]

# ê³¼ê±° ë©”ì‹œì§€ ì¶œë ¥
for msg in chat_history.messages:
    st.chat_message(msg.type).write(msg.content)

# ìœ ì € ì¸í’‹ ë°›ê¸°
if prompt_message := st.chat_input("Your question"):
    st.chat_message("human").write(prompt_message)
    with st.chat_message("ai"):
        with st.spinner("Thinking..."):
            config = {"configurable": {"session_id": "any"}}
            response = conversational_rag_chain.invoke({"input": prompt_message}, config)

            answer = response["answer"]
            st.write(answer)
            with st.expander("ì°¸ê³  ë¬¸ì„œ í™•ì¸"):
                for doc in response["context"]:
                    st.markdown(doc.metadata.get("source", ""), help=doc.page_content)
